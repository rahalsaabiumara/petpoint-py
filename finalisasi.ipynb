{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPKG2tanCEq2MBr0nSPyyQk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8fe1fe459f314529bbb1a5acb93c0694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Anda:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_dba7f36199cd4a6aaaed8f94329adc91",
            "placeholder": "Ketik pesan Anda...",
            "style": "IPY_MODEL_116fb15c013442aba8a85cd3ce926ed9",
            "value": ""
          }
        },
        "dba7f36199cd4a6aaaed8f94329adc91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "116fb15c013442aba8a85cd3ce926ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "84e3d69574594bcf94a92c797714ecc1": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_73f21b897c0e4c2ea998da06cefe79cd",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stdout",
                "text": [
                  "Anda: Kucing saya mengalami Gatal, kerontokan bulu, lesi kulit, penurunan berat badan\n",
                  "Chatbot: Kucing Anda terkena Infestasi Parasit. Pemberian antiparasit sesuai jenis parasit, menjaga kebersihan lingkungan, dan perawatan kulit yang tepat.\n",
                  "\n"
                ]
              }
            ]
          }
        },
        "73f21b897c0e4c2ea998da06cefe79cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bryanbayup/belajar-machine-learning/blob/main/finalisasi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.Persiapan Linkungan\n",
        "\n"
      ],
      "metadata": {
        "id": "HnLCWroLikBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install library yang diperlukan\n",
        "!pip install gensim\n",
        "!pip install tensorflow keras-tuner\n",
        "!pip install google-api-python-client\n",
        "!pip install seqeval\n",
        "!pip install imbalanced-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThnBuMzbiyz2",
        "outputId": "8753bb92-100d-49c8-e057-bd39b082154c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.8.30)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n",
            "Collecting google-api-python-client\n",
            "  Downloading google_api_python_client-2.154.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/lib/python3/dist-packages (from google-api-python-client) (0.20.2)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
            "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.23.0)\n",
            "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
            "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.8.30)\n",
            "Downloading google_api_python_client-2.154.0-py2.py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: uritemplate, google-auth-httplib2, google-api-python-client\n",
            "Successfully installed google-api-python-client-2.154.0 google-auth-httplib2-0.2.0 uritemplate-4.1.1\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=398c3e25f319963b9ed18e846e8c19d4862104044de9f12b1a6a9666fea2a914\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.12.4-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Downloading imbalanced_learn-0.12.4-py3-none-any.whl (258 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.3/258.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: imbalanced-learn\n",
            "Successfully installed imbalanced-learn-0.12.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import re\n",
        "import gensim\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from seqeval.metrics import classification_report as seq_classification_report\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from kerastuner import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import pickle\n",
        "import os\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "JFPi5b5nin1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27feb91e-1014-469f-bd90-6e6f488b9979"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-121ea54b41c5>:19: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
            "  from kerastuner import RandomSearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.Mount Google Drive"
      ],
      "metadata": {
        "id": "MqSfk0k5jKsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAkRahVVjNLa",
        "outputId": "c3f72b4d-00e7-417d-f89d-aeb7427ba1cb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.Download dan Persiapan FastText Embeddings"
      ],
      "metadata": {
        "id": "F0UDdqB0j-g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O id.tar.gz \"https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q58pSY49lyR",
        "outputId": "8371739d-7710-49d2-9a01-8a34d6791601"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-21 14:06:06--  https://www.dropbox.com/scl/fi/sju4o3keikox69euw51vy/id.tar.gz?rlkey=5jr3ijtbdwfahq7xcgig28qvy&e=1&st=gntzkzeo&dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.3.18, 2620:100:6018:18::a27d:312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.3.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc438b59fa65e31d10827c72d9d7.dl.dropboxusercontent.com/cd/0/inline/CeywAMzNy_ILvR6WxGAu1fKkbBwdTXr5X9UZ6ExSfbZgkzhLtEfN906_2mo4qO3v6lnYeG5_v_TqfBJW1saweGSVPkkMZEy_IkAplMRLZ4Iqb2nlfXb_HiD03edxFoIxa8o/file?dl=1# [following]\n",
            "--2024-11-21 14:06:07--  https://uc438b59fa65e31d10827c72d9d7.dl.dropboxusercontent.com/cd/0/inline/CeywAMzNy_ILvR6WxGAu1fKkbBwdTXr5X9UZ6ExSfbZgkzhLtEfN906_2mo4qO3v6lnYeG5_v_TqfBJW1saweGSVPkkMZEy_IkAplMRLZ4Iqb2nlfXb_HiD03edxFoIxa8o/file?dl=1\n",
            "Resolving uc438b59fa65e31d10827c72d9d7.dl.dropboxusercontent.com (uc438b59fa65e31d10827c72d9d7.dl.dropboxusercontent.com)... 162.125.3.15, 2620:100:6018:15::a27d:30f\n",
            "Connecting to uc438b59fa65e31d10827c72d9d7.dl.dropboxusercontent.com (uc438b59fa65e31d10827c72d9d7.dl.dropboxusercontent.com)|162.125.3.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CexdmqC-w4xPgR6mHG-l9Rez0T1UoFxrjftEW2MoOm8mIGp08ESH7gj2lVSEW0VRItcnVibU3udGueTUM7BAh-HI3URo1QaCqJktLKqiEN8SrmFS98rwAb4upbzMGlVVFupn0TTBQujMzhV7lcYOW2fAvOqo-Ux4PAaxdxqYwI4iYlmzQxk7Cm4DhiRWuyRUZBIUcn313WLZk34vXbLVSKyLe9rFe_Z6Vua86OYcUE4hNWwI_ceD75_4ltZEjYdlr9E-1EuWwIiJiTRlugIqgla9ecLs-NQm-UEofkHHbh_7TIOUqM1FpXt6Jp6i3DyUEzHXAhJXUEDgXT3nWIFDL4usZwr1msZpZIBMQt7bg1HuyQ/file?dl=1 [following]\n",
            "--2024-11-21 14:06:08--  https://uc438b59fa65e31d10827c72d9d7.dl.dropboxusercontent.com/cd/0/inline2/CexdmqC-w4xPgR6mHG-l9Rez0T1UoFxrjftEW2MoOm8mIGp08ESH7gj2lVSEW0VRItcnVibU3udGueTUM7BAh-HI3URo1QaCqJktLKqiEN8SrmFS98rwAb4upbzMGlVVFupn0TTBQujMzhV7lcYOW2fAvOqo-Ux4PAaxdxqYwI4iYlmzQxk7Cm4DhiRWuyRUZBIUcn313WLZk34vXbLVSKyLe9rFe_Z6Vua86OYcUE4hNWwI_ceD75_4ltZEjYdlr9E-1EuWwIiJiTRlugIqgla9ecLs-NQm-UEofkHHbh_7TIOUqM1FpXt6Jp6i3DyUEzHXAhJXUEDgXT3nWIFDL4usZwr1msZpZIBMQt7bg1HuyQ/file?dl=1\n",
            "Reusing existing connection to uc438b59fa65e31d10827c72d9d7.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2333351997 (2.2G) [application/binary]\n",
            "Saving to: ‘id.tar.gz’\n",
            "\n",
            "id.tar.gz           100%[===================>]   2.17G  88.2MB/s    in 28s     \n",
            "\n",
            "2024-11-21 14:06:36 (79.9 MB/s) - ‘id.tar.gz’ saved [2333351997/2333351997]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf id.tar.gz"
      ],
      "metadata": {
        "id": "EqC25jhEAW6E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# # URL FastText Bahasa Indonesia\n",
        "# #fasttext_url = 'https://www.dropbox.com/scl/fi/zjvzebgdklqosxpylu93b/id.tar.gz?rlkey=k8n322p8xteuog6lf3jdg2a6t&st=8wwkv788&dl=1'\n",
        "\n",
        "# # # Path untuk menyimpan di Google Drive\n",
        "# drive_path = '/content/drive/MyDrive/dataset'\n",
        "# os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "# # Unduh file dan simpan ke Google Drive\n",
        "# file_name = 'id.tar.gz'\n",
        "# drive_file_path = os.path.join(drive_path, file_name)\n",
        "# # !wget -O \"{drive_file_path}\" \"{fasttext_url}\"\n",
        "# print(f\"File telah diunduh dan disimpan di: {drive_file_path}\")\n",
        "\n",
        "# # Ekstrak file ke direktori aktif saat ini\n",
        "# !tar -xzf \"{drive_file_path}\" -C .\n",
        "# print(f\"File telah diekstrak ke direktori aktif: {os.getcwd()}\")\n",
        "\n",
        "# Muat model FastText dari file yang diekstrak\n",
        "try:\n",
        "    fasttext_model = KeyedVectors.load_word2vec_format('id.vec', binary=False)\n",
        "    print(\"Model FastText 'id.vec' berhasil dimuat.\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal memuat 'id.vec': {e}\")\n",
        "    # Jika gagal, coba muat 'id.bin'\n",
        "    try:\n",
        "        fasttext_model = KeyedVectors.load_facebook_vectors('id.bin')\n",
        "        print(\"Model FastText 'id.bin' berhasil dimuat.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal memuat 'id.bin': {e}\")\n",
        "        raise ValueError(\"Gagal memuat model FastText. Pastikan file 'id.vec' atau 'id.bin' dalam format yang benar.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvkLmdMUkALz",
        "outputId": "6353bf80-336c-4ab8-8074-5459fb5768ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model FastText 'id.vec' berhasil dimuat.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xzf \"{drive_file_path}\" -C .\n",
        "print(f\"File telah diekstrak ke direktori aktif: {os.getcwd()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jl02Pdi8-Q3",
        "outputId": "9c43adc3-c8d6-4de2-be1a-a5af9e45c982"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "gzip: stdin: not in gzip format\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n",
            "File telah diekstrak ke direktori aktif: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Preprocessing Data"
      ],
      "metadata": {
        "id": "lpWKm8EFlM_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Memuat dataset dari file JSON\n",
        "with open('dataaa.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Mengubah dataset menjadi DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Tampilakan 5 dataset paling atas\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "KF9xmLqdlPSK",
        "outputId": "012514e8-cde3-4b50-c913-21878b8855a7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                          utterances               intent  \\\n",
              "0  Anjing saya mengalami gatal-gatal terus meneru...  medical_inquiry_dog   \n",
              "1  Anjing saya terlihat sering muntah dan kehilan...  medical_inquiry_dog   \n",
              "2  Anjing saya terlihat lesu, demam, dan tidak ma...  medical_inquiry_dog   \n",
              "3  Anjing saya mengalami batuk kering dan nafasny...  medical_inquiry_dog   \n",
              "4  Anjing saya mengalami luka pada kulit yang men...  medical_inquiry_dog   \n",
              "\n",
              "                                            entities  \\\n",
              "0  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
              "1  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
              "2  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
              "3  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
              "4  [{'entity': 'animal', 'value': 'Anjing', 'star...   \n",
              "\n",
              "                                           responses  \n",
              "0  Gunakan sampo hipoalergenik, oleskan salep hid...  \n",
              "1  Berikan cairan elektrolit untuk mencegah dehid...  \n",
              "2  Pastikan anjing tetap terhidrasi, gunakan komp...  \n",
              "3  Berikan obat batuk khusus anjing yang disarank...  \n",
              "4  Bersihkan luka dengan antiseptik, oleskan sale...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ade2b584-921e-4c29-a54c-4ffdbacc2654\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>utterances</th>\n",
              "      <th>intent</th>\n",
              "      <th>entities</th>\n",
              "      <th>responses</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Anjing saya mengalami gatal-gatal terus meneru...</td>\n",
              "      <td>medical_inquiry_dog</td>\n",
              "      <td>[{'entity': 'animal', 'value': 'Anjing', 'star...</td>\n",
              "      <td>Gunakan sampo hipoalergenik, oleskan salep hid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Anjing saya terlihat sering muntah dan kehilan...</td>\n",
              "      <td>medical_inquiry_dog</td>\n",
              "      <td>[{'entity': 'animal', 'value': 'Anjing', 'star...</td>\n",
              "      <td>Berikan cairan elektrolit untuk mencegah dehid...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Anjing saya terlihat lesu, demam, dan tidak ma...</td>\n",
              "      <td>medical_inquiry_dog</td>\n",
              "      <td>[{'entity': 'animal', 'value': 'Anjing', 'star...</td>\n",
              "      <td>Pastikan anjing tetap terhidrasi, gunakan komp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Anjing saya mengalami batuk kering dan nafasny...</td>\n",
              "      <td>medical_inquiry_dog</td>\n",
              "      <td>[{'entity': 'animal', 'value': 'Anjing', 'star...</td>\n",
              "      <td>Berikan obat batuk khusus anjing yang disarank...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Anjing saya mengalami luka pada kulit yang men...</td>\n",
              "      <td>medical_inquiry_dog</td>\n",
              "      <td>[{'entity': 'animal', 'value': 'Anjing', 'star...</td>\n",
              "      <td>Bersihkan luka dengan antiseptik, oleskan sale...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ade2b584-921e-4c29-a54c-4ffdbacc2654')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ade2b584-921e-4c29-a54c-4ffdbacc2654 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ade2b584-921e-4c29-a54c-4ffdbacc2654');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bc718dcb-d5ae-49ca-8395-514e497bfa2d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bc718dcb-d5ae-49ca-8395-514e497bfa2d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bc718dcb-d5ae-49ca-8395-514e497bfa2d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 508,\n  \"fields\": [\n    {\n      \"column\": \"utterances\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 475,\n        \"samples\": [\n          \"Apakah kucing indoor perlu divaksin?\",\n          \"Anjing saya mengalami Diare, Nyeri perut, Gas berlebih. Apa yang harus saya lakukan?\",\n          \"Anjing saya mengalami kejang-kejang tadi malam. Apa yang harus saya lakukan?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"intent\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"intro_chat\",\n          \"pet_food_suggestion\",\n          \"medical_inquiry_dog\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"entities\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"responses\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 473,\n        \"samples\": [\n          \"Pastikan kucing tidak stres dan jauhkan dari udara panas. Ini bisa jadi tanda asma atau infeksi. Segera periksakan ke dokter hewan.\",\n          \"Cek kemungkinan iritasi kulit atau stres. Gunakan salep yang direkomendasikan dokter hewan untuk mengatasi iritasi.\",\n          \"Ini bisa menjadi tanda retensi cairan atau gangguan hormon. Periksakan ke dokter hewan untuk diagnosis lebih lanjut.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menentukan jumlah sampel maksimum per intent\n",
        "max_samples = 50\n",
        "\n",
        "df_list = []\n",
        "for intent in df['intent'].unique():\n",
        "    df_intent = df[df['intent'] == intent]\n",
        "    if len(df_intent) > max_samples:\n",
        "        df_intent = resample(df_intent, replace=False, n_samples=max_samples, random_state=42)\n",
        "    df_list.append(df_intent)\n",
        "\n",
        "df_balanced = pd.concat(df_list).reset_index(drop=True)\n",
        "\n",
        "# Encode intents\n",
        "label_encoder = LabelEncoder()\n",
        "df_balanced['intent_label'] = label_encoder.fit_transform(df_balanced['intent'])\n",
        "\n",
        "# Simpan mapping label\n",
        "intent_mapping = dict(zip(df_balanced['intent_label'], df_balanced['intent']))\n",
        "\n",
        "# Mengatasi kelas imbalanced dengan oversampling\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "\n",
        "X_ros, y_ros = ros.fit_resample(df_balanced[['utterances', 'responses']], df_balanced['intent_label'])\n",
        "\n",
        "df_balanced = pd.DataFrame({\n",
        "    'utterances': X_ros['utterances'],\n",
        "    'responses': X_ros['responses'],\n",
        "    'intent_label': y_ros\n",
        "})\n",
        "\n",
        "# Filter data yang memiliki entitas\n",
        "df_ner = df[df['entities'].map(lambda d: len(d)) > 0].reset_index(drop=True)\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Pembersihan teks\n",
        "df_balanced['utterances_clean'] = df_balanced['utterances'].apply(clean_text)\n",
        "df_ner['utterances_clean'] = df_ner['utterances'].apply(clean_text)\n",
        "\n",
        "texts = df_balanced['utterances_clean'].tolist()\n",
        "labels = df_balanced['intent_label'].tolist()\n",
        "\n",
        "# Split data untuk Klasifikasi Intent\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    texts,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")"
      ],
      "metadata": {
        "id": "lanqaQVVmO25"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Tokenisasi dan Pembuatan Embedding Matrix"
      ],
      "metadata": {
        "id": "UIgPUWGTlo9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisasi\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "# Mengonversi teks ke sequences\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "\n",
        "# Padding sequences\n",
        "max_seq_length = max(max(len(seq) for seq in train_sequences), max(len(seq) for seq in val_sequences))\n",
        "train_padded = pad_sequences(train_sequences, maxlen=max_seq_length, padding='post')\n",
        "val_padded = pad_sequences(val_sequences, maxlen=max_seq_length, padding='post')\n",
        "\n",
        "# Mengonversi labels ke categorical\n",
        "num_classes = len(label_encoder.classes_)\n",
        "train_labels_cat = to_categorical(train_labels, num_classes=num_classes)\n",
        "val_labels_cat = to_categorical(val_labels, num_classes=num_classes)\n",
        "\n",
        "# Membuat embedding matrix menggunakan FastText\n",
        "embedding_dim = fasttext_model.vector_size\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in word_index.items():\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix[idx] = fasttext_model[word]\n",
        "    else:\n",
        "        embedding_matrix[idx] = np.random.normal(scale=0.6, size=(embedding_dim,))"
      ],
      "metadata": {
        "id": "aLm2itWMlrkw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.Persiapan Data untuk NER"
      ],
      "metadata": {
        "id": "DEeEpIaUl4xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_ner_data(df, tokenizer, max_seq_length):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    for index, row in df.iterrows():\n",
        "        text = row['utterances_clean']\n",
        "        entities = row['entities']\n",
        "        tokens = tokenizer.texts_to_sequences([text])[0]\n",
        "        label_seq = ['O'] * len(tokens)\n",
        "        for ent in entities:\n",
        "            ent_text = clean_text(ent['value'])\n",
        "            ent_tokens = tokenizer.texts_to_sequences([ent_text])[0]\n",
        "            ent_len = len(ent_tokens)\n",
        "            for i in range(len(tokens) - ent_len + 1):\n",
        "                if tokens[i:i+ent_len] == ent_tokens:\n",
        "                    label_seq[i] = 'B-' + ent['entity']\n",
        "                    for j in range(1, ent_len):\n",
        "                        label_seq[i+j] = 'I-' + ent['entity']\n",
        "                    break\n",
        "        texts.append(tokens)\n",
        "        labels.append(label_seq)\n",
        "    # Padding\n",
        "    texts_padded = pad_sequences(texts, maxlen=max_seq_length, padding='post')\n",
        "    # Padding labels\n",
        "    labels_padded = [label + ['O']*(max_seq_length - len(label)) for label in labels]\n",
        "    return texts_padded, labels_padded\n",
        "\n",
        "# Membuat label encoder untuk NER\n",
        "all_labels = set()\n",
        "for label_list in df_ner['entities']:\n",
        "    for ent in label_list:\n",
        "        all_labels.add('B-' + ent['entity'])\n",
        "        all_labels.add('I-' + ent['entity'])\n",
        "all_labels.add('O')\n",
        "ner_label_encoder = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
        "ner_label_decoder = {idx: label for label, idx in ner_label_encoder.items()}\n",
        "\n",
        "# Siapkan data NER\n",
        "texts_ner, labels_ner = prepare_ner_data(df_ner, tokenizer, max_seq_length)\n",
        "\n",
        "# Mengonversi labels ke format numerik dan categorical\n",
        "def encode_ner_labels(labels, ner_label_encoder):\n",
        "    labels_encoded = []\n",
        "    for label_seq in labels:\n",
        "        label_ids = [ner_label_encoder[label] for label in label_seq]\n",
        "        labels_encoded.append(label_ids)\n",
        "    labels_encoded = np.array(labels_encoded)\n",
        "    labels_encoded = to_categorical(labels_encoded, num_classes=len(ner_label_encoder))\n",
        "    return labels_encoded\n",
        "\n",
        "labels_ner_encoded = encode_ner_labels(labels_ner, ner_label_encoder)\n",
        "\n",
        "# Split data untuk NER\n",
        "train_texts_ner, val_texts_ner, train_labels_ner, val_labels_ner = train_test_split(\n",
        "    texts_ner,\n",
        "    labels_ner_encoded,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        ")"
      ],
      "metadata": {
        "id": "hlGrsvh7l7hp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.Definisi dan Kompilasi Model"
      ],
      "metadata": {
        "id": "Y2MmqnzYm7Ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Input, Dropout, Bidirectional, LSTM, TimeDistributed\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Definisikan Fungsi untuk Membangun Model Klasifikasi Intent\n",
        "def build_intent_model(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=embedding_matrix.shape[0],\n",
        "        output_dim=embedding_matrix.shape[1],\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_length,\n",
        "        trainable=False\n",
        "    )(inputs)\n",
        "    lstm = Bidirectional(LSTM(64, kernel_regularizer=l2(l2_reg), return_sequences=False))(embedding)\n",
        "    dense = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(lstm)\n",
        "    dropout = Dropout(0.5)(dense)\n",
        "    outputs = Dense(num_classes, activation='softmax')(dropout)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Definisikan Fungsi untuk Membangun Model NER\n",
        "def build_ner_model(embedding_matrix, max_seq_length, num_entities, l2_reg=0.001):\n",
        "    inputs = Input(shape=(max_seq_length,))\n",
        "    embedding = tf.keras.layers.Embedding(\n",
        "        input_dim=embedding_matrix.shape[0],\n",
        "        output_dim=embedding_matrix.shape[1],\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_seq_length,\n",
        "        trainable=False\n",
        "    )(inputs)\n",
        "    lstm = Bidirectional(LSTM(64, kernel_regularizer=l2(l2_reg), return_sequences=True))(embedding)\n",
        "    dropout = Dropout(0.5)(lstm)\n",
        "    outputs = TimeDistributed(Dense(num_entities, activation='softmax', kernel_regularizer=l2(l2_reg)))(dropout)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Membangun Model Klasifikasi Intent\n",
        "model_intent = build_intent_model(embedding_matrix, max_seq_length, num_classes, l2_reg=0.001)\n",
        "model_intent.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_intent.summary()\n",
        "\n",
        "# Membangun Model NER\n",
        "model_ner = build_ner_model(embedding_matrix, max_seq_length, len(ner_label_encoder), l2_reg=0.001)\n",
        "model_ner.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_ner.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M234R5_3m9V9",
        "outputId": "9850f0d0-b480-4e43-db63-739feaeb9c67"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 28)]              0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 28, 300)           185100    \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 128)               186880    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 381211 (1.45 MB)\n",
            "Trainable params: 196111 (766.06 KB)\n",
            "Non-trainable params: 185100 (723.05 KB)\n",
            "_________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 28)]              0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 28, 300)           185100    \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 28, 128)           186880    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 28, 128)           0         \n",
            "                                                                 \n",
            " time_distributed (TimeDist  (None, 28, 17)            2193      \n",
            " ributed)                                                        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 374173 (1.43 MB)\n",
            "Trainable params: 189073 (738.57 KB)\n",
            "Non-trainable params: 185100 (723.05 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.Pelatihan Model dengan Hyperparameter Optimization"
      ],
      "metadata": {
        "id": "lkYntYqbnNQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kerastuner import HyperModel\n",
        "\n",
        "# Definisikan HyperModel untuk Intent Classification\n",
        "class IntentHyperModel(HyperModel):\n",
        "    def __init__(self, embedding_matrix, max_seq_length, num_classes):\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        l2_reg = hp.Choice('l2_reg', values=[1e-4, 1e-3, 1e-2])\n",
        "        dropout_rate = hp.Float('dropout_rate', 0.3, 0.7, step=0.1)\n",
        "        lstm_units = hp.Int('lstm_units', min_value=32, max_value=128, step=32)\n",
        "        dense_units = hp.Int('dense_units', min_value=32, max_value=128, step=32)\n",
        "\n",
        "        inputs = Input(shape=(self.max_seq_length,))\n",
        "        embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=self.embedding_matrix.shape[0],\n",
        "            output_dim=self.embedding_matrix.shape[1],\n",
        "            weights=[self.embedding_matrix],\n",
        "            input_length=self.max_seq_length,\n",
        "            trainable=False\n",
        "        )(inputs)\n",
        "        lstm = Bidirectional(LSTM(lstm_units, kernel_regularizer=l2(l2_reg), return_sequences=False))(embedding)\n",
        "        dense = Dense(dense_units, activation='relu', kernel_regularizer=l2(l2_reg))(lstm)\n",
        "        dropout = Dropout(dropout_rate)(dense)\n",
        "        outputs = Dense(self.num_classes, activation='softmax')(dropout)\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "        return model\n",
        "\n",
        "# Inisialisasi HyperModel\n",
        "intent_hypermodel = IntentHyperModel(embedding_matrix, max_seq_length, num_classes)\n",
        "\n",
        "# Inisialisasi RandomSearch\n",
        "tuner_intent = RandomSearch(\n",
        "    intent_hypermodel,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=10,\n",
        "    executions_per_trial=2,\n",
        "    directory='intent_tuner_dir',\n",
        "    project_name='intent_classification'\n",
        ")\n",
        "\n",
        "# Menampilkan ringkasan tuner\n",
        "tuner_intent.search_space_summary()\n",
        "\n",
        "# Pencarian Hyperparameter untuk Intent Classification\n",
        "tuner_intent.search(\n",
        "    train_padded,\n",
        "    train_labels_cat,\n",
        "    epochs=10,\n",
        "    validation_data=(val_padded, val_labels_cat),\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Mendapatkan model terbaik untuk Intent Classification\n",
        "best_model_intent = tuner_intent.get_best_models(num_models=1)[0]\n",
        "best_hp_intent = tuner_intent.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(f\"Best Hyperparameters for Intent Classification: {best_hp_intent.values}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUFiXYzInPXg",
        "outputId": "8203c6af-4f9f-453b-faae-10be49f500c8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 16s]\n",
            "val_accuracy: 0.9399999976158142\n",
            "\n",
            "Best val_accuracy So Far: 0.9699999988079071\n",
            "Total elapsed time: 00h 03m 10s\n",
            "Best Hyperparameters for Intent Classification: {'l2_reg': 0.001, 'dropout_rate': 0.6000000000000001, 'lstm_units': 64, 'dense_units': 96}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.Pelatihan Model NER"
      ],
      "metadata": {
        "id": "_LeE8EVdnh0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mempersiapkan Callback untuk NER\n",
        "callbacks_ner = [\n",
        "    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    ModelCheckpoint(\n",
        "        filepath='best_model_ner.keras',\n",
        "        save_best_only=True,\n",
        "        save_weights_only=False\n",
        "    )\n",
        "]\n",
        "\n",
        "# Melatih Model NER\n",
        "history_ner = model_ner.fit(\n",
        "    train_texts_ner,\n",
        "    train_labels_ner,\n",
        "    validation_data=(val_texts_ner, val_labels_ner),\n",
        "    epochs=20,\n",
        "    batch_size=16,\n",
        "    callbacks=callbacks_ner\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JA8WOiz9nkIn",
        "outputId": "fbe70edc-8e91-4024-8ff6-34ec76521d8b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "23/23 [==============================] - 4s 52ms/step - loss: 2.2894 - accuracy: 0.7502 - val_loss: 1.1493 - val_accuracy: 0.8188\n",
            "Epoch 2/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.9748 - accuracy: 0.8297 - val_loss: 0.7914 - val_accuracy: 0.8652\n",
            "Epoch 3/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.6983 - accuracy: 0.8834 - val_loss: 0.6445 - val_accuracy: 0.8848\n",
            "Epoch 4/20\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.5710 - accuracy: 0.8923 - val_loss: 0.5494 - val_accuracy: 0.8920\n",
            "Epoch 5/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.5028 - accuracy: 0.9002 - val_loss: 0.4824 - val_accuracy: 0.8991\n",
            "Epoch 6/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.4401 - accuracy: 0.9066 - val_loss: 0.4371 - val_accuracy: 0.9036\n",
            "Epoch 7/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.4056 - accuracy: 0.9096 - val_loss: 0.4093 - val_accuracy: 0.9098\n",
            "Epoch 8/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.3769 - accuracy: 0.9132 - val_loss: 0.3795 - val_accuracy: 0.9143\n",
            "Epoch 9/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.3449 - accuracy: 0.9202 - val_loss: 0.3587 - val_accuracy: 0.9161\n",
            "Epoch 10/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.3268 - accuracy: 0.9226 - val_loss: 0.3441 - val_accuracy: 0.9161\n",
            "Epoch 11/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.3055 - accuracy: 0.9286 - val_loss: 0.3238 - val_accuracy: 0.9250\n",
            "Epoch 12/20\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2896 - accuracy: 0.9323 - val_loss: 0.3102 - val_accuracy: 0.9277\n",
            "Epoch 13/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2764 - accuracy: 0.9336 - val_loss: 0.3045 - val_accuracy: 0.9268\n",
            "Epoch 14/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2640 - accuracy: 0.9377 - val_loss: 0.2920 - val_accuracy: 0.9304\n",
            "Epoch 15/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2509 - accuracy: 0.9431 - val_loss: 0.2898 - val_accuracy: 0.9312\n",
            "Epoch 16/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2493 - accuracy: 0.9419 - val_loss: 0.2782 - val_accuracy: 0.9357\n",
            "Epoch 17/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2362 - accuracy: 0.9473 - val_loss: 0.2719 - val_accuracy: 0.9366\n",
            "Epoch 18/20\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.2293 - accuracy: 0.9487 - val_loss: 0.2598 - val_accuracy: 0.9384\n",
            "Epoch 19/20\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.2374 - accuracy: 0.9438 - val_loss: 0.2689 - val_accuracy: 0.9375\n",
            "Epoch 20/20\n",
            "23/23 [==============================] - 0s 18ms/step - loss: 0.2251 - accuracy: 0.9488 - val_loss: 0.2580 - val_accuracy: 0.9375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi Klasifikasi Intent\n",
        "loss_intent, accuracy_intent = best_model_intent.evaluate(val_padded, val_labels_cat)\n",
        "print(f'Akurasi Model Klasifikasi Intent: {accuracy_intent * 100:.2f}%')\n",
        "\n",
        "# Prediksi pada data validasi Intent\n",
        "val_preds_intent = best_model_intent.predict(val_padded)\n",
        "val_preds_intent = np.argmax(val_preds_intent, axis=1)\n",
        "val_true_intent = np.argmax(val_labels_cat, axis=1)\n",
        "\n",
        "# Classification report untuk Intent\n",
        "print(\"Classification Report untuk Intent:\")\n",
        "print(classification_report(val_true_intent, val_preds_intent, target_names=label_encoder.classes_))\n",
        "\n",
        "# Evaluasi NER\n",
        "loss_ner, accuracy_ner = model_ner.evaluate(val_texts_ner, val_labels_ner)\n",
        "print(f'Akurasi Model NER: {accuracy_ner * 100:.2f}%')\n",
        "\n",
        "# Prediksi pada data validasi NER\n",
        "val_preds_ner = model_ner.predict(val_texts_ner)\n",
        "val_preds_ner = np.argmax(val_preds_ner, axis=-1)\n",
        "val_true_ner = np.argmax(val_labels_ner, axis=-1)\n",
        "\n",
        "# Konversi label ke format aslinya\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for i in range(len(val_preds_ner)):\n",
        "    true_label = []\n",
        "    pred_label = []\n",
        "    for j in range(len(val_preds_ner[i])):\n",
        "        true_l = ner_label_decoder[val_true_ner[i][j]]\n",
        "        pred_l = ner_label_decoder[val_preds_ner[i][j]]\n",
        "        if true_l != 'O':\n",
        "            true_label.append(true_l)\n",
        "            pred_label.append(pred_l)\n",
        "    true_labels.append(true_label)\n",
        "    pred_labels.append(pred_label)\n",
        "\n",
        "# Classification report untuk NER\n",
        "print(\"Classification Report untuk NER:\")\n",
        "print(seq_classification_report(true_labels, pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzID2bPznnk2",
        "outputId": "d2dd349b-6ede-41e7-ddbf-11672f5f606a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 8ms/step - loss: 0.4276 - accuracy: 0.9800\n",
            "Akurasi Model Klasifikasi Intent: 98.00%\n",
            "5/5 [==============================] - 1s 8ms/step\n",
            "Classification Report untuk Intent:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "         cat_healthcare       1.00      1.00      1.00        10\n",
            " disease_prevention_cat       1.00      1.00      1.00        10\n",
            " disease_prevention_dog       1.00      1.00      1.00        10\n",
            "         dog_healthcare       1.00      0.90      0.95        10\n",
            "               end_chat       1.00      1.00      1.00        10\n",
            "             intro_chat       1.00      1.00      1.00        10\n",
            " medical_inquiry_anjing       1.00      1.00      1.00        10\n",
            "    medical_inquiry_cat       0.83      1.00      0.91        10\n",
            "    medical_inquiry_dog       1.00      1.00      1.00        10\n",
            " medical_inquiry_kucing       1.00      0.90      0.95        10\n",
            "    pet_food_suggestion       0.91      1.00      0.95        10\n",
            "pet_health_status_check       1.00      1.00      1.00        10\n",
            "  pet_vaccination_check       1.00      1.00      1.00        10\n",
            "   symptom_analysis_cat       1.00      0.90      0.95        10\n",
            "   symptom_analysis_dog       1.00      1.00      1.00        10\n",
            "\n",
            "               accuracy                           0.98       150\n",
            "              macro avg       0.98      0.98      0.98       150\n",
            "           weighted avg       0.98      0.98      0.98       150\n",
            "\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2580 - accuracy: 0.9375\n",
            "Akurasi Model NER: 93.75%\n",
            "2/2 [==============================] - 1s 8ms/step\n",
            "Classification Report untuk NER:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      animal       0.97      0.97      0.97        36\n",
            "   condition       0.00      0.00      0.00         1\n",
            "     disease       0.25      0.20      0.22         5\n",
            "    duration       0.00      0.00      0.00         3\n",
            "        food       0.00      0.00      0.00         2\n",
            "     symptom       0.53      0.44      0.48        57\n",
            "\n",
            "   micro avg       0.70      0.59      0.64       104\n",
            "   macro avg       0.29      0.27      0.28       104\n",
            "weighted avg       0.64      0.59      0.61       104\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Konfigurasi Google Custom Search API\n",
        "GOOGLE_API_KEY = 'AIzaSyD-f-0S98J0bcdG_5AvbuNGSBVHIgQIX1Y'\n",
        "GOOGLE_CSE_ID = '30b6a924536894083'\n",
        "\n",
        "def google_search(query):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_API_KEY)\n",
        "    res = service.cse().list(q=query, cx=GOOGLE_CSE_ID, num=1).execute()\n",
        "    results = res.get('items', [])\n",
        "    if results:\n",
        "        snippet = results[0].get('snippet', '')\n",
        "        return snippet\n",
        "    else:\n",
        "        return \"Maaf, saya tidak menemukan informasi yang Anda cari.\""
      ],
      "metadata": {
        "id": "VY6ok0OxoqhN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Membuat DataFrame utterances dan responses\n",
        "df_utterances = df_balanced[['utterances', 'responses']].reset_index(drop=True)\n",
        "df_utterances['utterances_clean'] = df_utterances['utterances'].apply(clean_text)\n",
        "\n",
        "# Menghitung TF-IDF matrix\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(df_utterances['utterances_clean'])\n",
        "\n",
        "def predict_intent(text):\n",
        "    text_clean = clean_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = best_model_intent.predict(padded_seq)\n",
        "    predicted_label = np.argmax(pred, axis=1)[0]\n",
        "    intent = label_encoder.inverse_transform([predicted_label])[0]\n",
        "    return intent\n",
        "\n",
        "def predict_entities(text):\n",
        "    text_clean = clean_text(text)\n",
        "    seq = tokenizer.texts_to_sequences([text_clean])\n",
        "    padded_seq = pad_sequences(seq, maxlen=max_seq_length, padding='post')\n",
        "    pred = model_ner.predict(padded_seq)\n",
        "    pred_labels = np.argmax(pred, axis=-1)[0]\n",
        "    tokens = tokenizer.sequences_to_texts(seq)[0].split()\n",
        "    entities = []\n",
        "    for idx, label_id in enumerate(pred_labels[:len(tokens)]):\n",
        "        label = ner_label_decoder[label_id]\n",
        "        if label != 'O':\n",
        "            entities.append({'entity': label.split('-')[1], 'value': tokens[idx]})\n",
        "    return entities\n",
        "\n",
        "def get_response(user_input):\n",
        "    # Preprocess input\n",
        "    user_input_clean = clean_text(user_input)\n",
        "    user_tfidf = vectorizer.transform([user_input_clean])\n",
        "\n",
        "    # Hitung cosine similarity\n",
        "    similarities = cosine_similarity(user_tfidf, tfidf_matrix)\n",
        "\n",
        "    # Dapatkan indeks dengan similarity tertinggi\n",
        "    most_similar_idx = np.argmax(similarities[0])\n",
        "\n",
        "    # Dapatkan nilai similarity tertinggi\n",
        "    highest_similarity = similarities[0][most_similar_idx]\n",
        "\n",
        "    # Jika similarity rendah, gunakan RAG\n",
        "    if highest_similarity < 0.2:\n",
        "        search_result = google_search(user_input)\n",
        "        return f\"{search_result}\"\n",
        "    else:\n",
        "        # Ambil respon yang sesuai\n",
        "        response = df_utterances.iloc[most_similar_idx]['responses']\n",
        "        return response\n",
        "\n",
        "def chatbot_response(user_input):\n",
        "    # Prediksi intent dan entitas\n",
        "    intent = predict_intent(user_input)\n",
        "    entities = predict_entities(user_input)\n",
        "\n",
        "    # Respon berbasis TF-IDF atau RAG\n",
        "    response = get_response(user_input)\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "NyEQAxovpE0l"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Membuat widget input dan output\n",
        "input_box = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Ketik pesan Anda...',\n",
        "    description='Anda:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "output_area = widgets.Output()\n",
        "\n",
        "def on_submit(sender):\n",
        "    user_input = input_box.value\n",
        "    input_box.value = ''\n",
        "    response = chatbot_response(user_input)\n",
        "    with output_area:\n",
        "        clear_output()\n",
        "        print(f\"Anda: {user_input}\")\n",
        "        print(f\"Chatbot: {response}\\n\")\n",
        "\n",
        "input_box.on_submit(on_submit)\n",
        "\n",
        "display(input_box, output_area)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293,
          "referenced_widgets": [
            "8fe1fe459f314529bbb1a5acb93c0694",
            "dba7f36199cd4a6aaaed8f94329adc91",
            "116fb15c013442aba8a85cd3ce926ed9",
            "84e3d69574594bcf94a92c797714ecc1",
            "73f21b897c0e4c2ea998da06cefe79cd"
          ]
        },
        "id": "QdMzSS6dpUHH",
        "outputId": "deffcff6-7052-430b-edd3-5fbd8c6db10f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Anda:', placeholder='Ketik pesan Anda...')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fe1fe459f314529bbb1a5acb93c0694"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84e3d69574594bcf94a92c797714ecc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan model intent\n",
        "best_model_intent.save('models/model_intent.keras')\n",
        "\n",
        "# Simpan model NER\n",
        "model_ner.save('models/model_ner.keras')"
      ],
      "metadata": {
        "id": "aert1BBCpXjF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Simpan tokenizer\n",
        "with open('encoders/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Simpan label encoder\n",
        "with open('encoders/label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Simpan NER label encoder\n",
        "with open('encoders/ner_label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(ner_label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "Co2BtvLIEpH4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan DataFrame df_utterances\n",
        "df_utterances.to_pickle('data/df_utterances.pkl')\n",
        "\n",
        "# Simpan vectorizer\n",
        "with open('data/vectorizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Simpan TF-IDF matrix\n",
        "with open('data/tfidf_matrix.pickle', 'wb') as handle:\n",
        "    pickle.dump(tfidf_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ],
      "metadata": {
        "id": "V7s12sw9E2AL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r chatbot.zip models/ data/ encoders/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvAspCjkFCGy",
        "outputId": "70b59c45-6b64-4548-dce3-828e62118117"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: models/ (stored 0%)\n",
            "  adding: models/model_ner.keras (deflated 8%)\n",
            "  adding: models/model_intent.keras (deflated 10%)\n",
            "  adding: data/ (stored 0%)\n",
            "  adding: data/tfidf_matrix.pickle (deflated 49%)\n",
            "  adding: data/df_utterances.pkl (deflated 80%)\n",
            "  adding: data/vectorizer.pickle (deflated 54%)\n",
            "  adding: encoders/ (stored 0%)\n",
            "  adding: encoders/ner_label_encoder.pickle (deflated 36%)\n",
            "  adding: encoders/label_encoder.pickle (deflated 38%)\n",
            "  adding: encoders/tokenizer.pickle (deflated 46%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_intent.save('models/model_intent.keras', save_format='h5')\n",
        "model_ner.save('models/model_ner.keras', save_format='h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWvshTkoFour",
        "outputId": "93408fc5-949d-4e15-ffe0-92c84e876f5c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j94nYMKBJiKr",
        "outputId": "89dcfca4-e65d-4380-da5c-d13b434ef8ab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import importlib\n",
        "\n",
        "libraries = [\n",
        "    \"json\", \"pandas\", \"numpy\", \"tensorflow\", \"gensim\",\n",
        "    \"sklearn\", \"seqeval\", \"kerastuner\", \"imblearn\", \"pickle\", \"os\"\n",
        "]\n",
        "\n",
        "for lib in libraries:\n",
        "    try:\n",
        "        module = importlib.import_module(lib)\n",
        "        version = getattr(module, \"__version__\", \"Version info not available\")\n",
        "        print(f\"{lib}: {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"{lib}: Not installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHnmOFHCJvhO",
        "outputId": "3ce867fb-d85d-4683-9fb8-eda52b1623e5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "json: 2.0.9\n",
            "pandas: 2.2.2\n",
            "numpy: 1.26.4\n",
            "tensorflow: 2.15.0\n",
            "gensim: 4.3.3\n",
            "sklearn: 1.5.2\n",
            "seqeval: Version info not available\n",
            "kerastuner: 1.0.5\n",
            "imblearn: 0.12.4\n",
            "pickle: Version info not available\n",
            "os: Version info not available\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setelah melatih model di Colab\n",
        "\n",
        "# Menyimpan model dengan tf.keras\n",
        "best_model_intent.save('app/models/model_intent.keras', save_format='h5')  # Atau 'tf'\n",
        "model_ner.save('app/models/model_ner.keras', save_format='h5')            # Atau 'tf'\n",
        "\n",
        "# Menyimpan tokenizer dan encoders\n",
        "with open('app/encoders/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('app/encoders/label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('app/encoders/ner_label_encoder.pickle', 'wb') as handle:\n",
        "    pickle.dump(ner_label_encoder, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Menyimpan vectorizer dan TF-IDF matrix\n",
        "with open('app/data/vectorizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(vectorizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "with open('app/data/tfidf_matrix.pickle', 'wb') as handle:\n",
        "    pickle.dump(tfidf_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Menyimpan DataFrame respons\n",
        "df_utterances.to_pickle('app/data/df_utterances.pkl')"
      ],
      "metadata": {
        "id": "O5jhN3m4KW9V"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r all.zip app/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqmf4OPJM5Ib",
        "outputId": "e3828899-326e-4284-87f8-ff0519003223"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: app/ (stored 0%)\n",
            "  adding: app/models/ (stored 0%)\n",
            "  adding: app/models/model_ner.keras (deflated 8%)\n",
            "  adding: app/models/model_intent.keras (deflated 9%)\n",
            "  adding: app/data/ (stored 0%)\n",
            "  adding: app/data/tfidf_matrix.pickle (deflated 49%)\n",
            "  adding: app/data/df_utterances.pkl (deflated 80%)\n",
            "  adding: app/data/vectorizer.pickle (deflated 54%)\n",
            "  adding: app/encoders/ (stored 0%)\n",
            "  adding: app/encoders/ner_label_encoder.pickle (deflated 36%)\n",
            "  adding: app/encoders/label_encoder.pickle (deflated 38%)\n",
            "  adding: app/encoders/tokenizer.pickle (deflated 46%)\n",
            "  adding: app/.ipynb_checkpoints/ (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-cg3C77TNECv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}